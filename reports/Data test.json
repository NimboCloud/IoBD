{"paragraphs":[{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466792499251_1844378646","id":"20160624-182139_1112872780","dateCreated":"Jun 24, 2016 6:21:39 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:334","text":"import org.apache.commons.io.IOUtils\r\nimport java.net.URL\r\nimport java.nio.charset.Charset\r\n\r\n// Zeppelin creates and injects sc (SparkContext) and sqlContext (HiveContext or SqlContext)\r\n// So you don't need create them manually\r\n\r\n// load bank data\r\nval bankText = sc.parallelize(\r\n    IOUtils.toString(\r\n        new URL(\"https://s3.amazonaws.com/apache-zeppelin/tutorial/bank/bank.csv\"),\r\n        Charset.forName(\"utf8\")).split(\"\\n\"))\r\n\r\nval copaText = sc.parallelize(\r\n    IOUtils.toString(\r\n        new URL(\"https://s3.amazonaws.com/bdt-iobd-blog/copa-america-historical/copa_america_historical_data.csv\"),\r\n        Charset.forName(\"utf8\")).split(\"\\n\"))\r\n//https://s3.amazonaws.com/bdt-iobd-blog/copa-america-historical/copa_america_historical_data.csv\r\n\r\ncase class Bank(age: Integer, job: String, marital: String, education: String, balance: Integer)\r\n\r\nval bank = bankText.map(s => s.split(\";\")).filter(s => s(0) != \"\\\"age\\\"\").map(\r\n    s => Bank(s(0).toInt, \r\n            s(1).replaceAll(\"\\\"\", \"\"),\r\n            s(2).replaceAll(\"\\\"\", \"\"),\r\n            s(3).replaceAll(\"\\\"\", \"\"),\r\n            s(5).replaceAll(\"\\\"\", \"\").toInt\r\n        )\r\n).toDF()\r\nbank.registerTempTable(\"bank\")","dateUpdated":"Jun 24, 2016 6:27:24 PM","dateFinished":"Jun 24, 2016 6:27:27 PM","dateStarted":"Jun 24, 2016 6:27:24 PM","result":{"code":"SUCCESS","type":"TEXT","msg":"import org.apache.commons.io.IOUtils\nimport java.net.URL\nimport java.nio.charset.Charset\nbankText: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[32] at parallelize at <console>:41\ncopaText: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[33] at parallelize at <console>:37\ndefined class Bank\nbank: org.apache.spark.sql.DataFrame = [age: int, job: string, marital: string, education: string, balance: int]\n"}},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466792844769_1222530834","id":"20160624-182724_691294140","dateCreated":"Jun 24, 2016 6:27:24 PM","status":"READY","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:359"}],"name":"Data test","id":"2BPGH7SA4","angularObjects":{"2B44YVSN1":[],"2AJXGMUUJ":[],"2AK8P7CPX":[],"2AM1YV5CU":[],"2AKK3QQXU":[],"2ANGGHHMQ":[]},"config":{"looknfeel":"default"},"info":{}}